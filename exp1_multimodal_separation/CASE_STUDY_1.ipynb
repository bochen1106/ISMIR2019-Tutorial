{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 1: Audio source separation using motion information\n",
    "\n",
    "To run this jupyter notebook you would require: python, GNU octave and jupyter. Creating a separate environment in Miniconda/Anaconda is highly recommended. Please find below a detailed set of steps to get started. If you already satisfy all of the software requirements, skip to step 6.\n",
    "\n",
    "\n",
    "   1. Install GNU Octave: https://www.gnu.org/software/octave/download.html. Here are some ways to do so:\n",
    "      - Linux (Package manager): ```apt install octave or yum install octave```\n",
    "      - Linux or MacOS (Homebrew): ```brew install octave```\n",
    "      - Windows: Get installer from aforementioned download link\n",
    "   2. Install Miniconda: https://docs.conda.io/en/latest/miniconda.html\n",
    "   3. Create a new conda environment using ```environment.yml``` file\n",
    "      ```sh\n",
    "      conda env create -f environment.yml\n",
    "      ```\n",
    "   4. Activate the environment\n",
    "      ```sh\n",
    "      conda activate ismir2019\n",
    "      ```\n",
    "   5. Get the code and enter the case study directory\n",
    "      ```sh\n",
    "      git clone <repository_link>\n",
    "      cd <case_study_directory>\n",
    "      ```\n",
    "   6. If you have not used ```environment.yml``` please verify that you have all packages listed in that file installed, especially ```oct2py``` python package.\n",
    "      ```sh\n",
    "      pip install oct2py\n",
    "      ```\n",
    "      or\n",
    "      ```sh\n",
    "      conda install -c conda-forge oct2py\n",
    "      ```\n",
    "      \n",
    "   7. Start the jupyter server and open the notebook\n",
    "      ```sh\n",
    "      jupyter notebook\n",
    "      ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio source separation using visual motion information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several studies in the field of neuroscience have established how the correlations between audio and motion  are utilized by the human brain to segregate sound sources, especially in noisy environments (Krishnan et al., 2014). This notebook provides a look into a particular multimodal formulation within the NMF framework that aims to segregate audio sources using corresponding motion information. Herein the cost functions for audio factorization and cross-modal sparse nonnegative least squares are simultaneously minimized. \n",
    "\n",
    "$$\\newcommand{\\reals}{\\mathbb{R}}\n",
    "\\newcommand{\\Va}{{\\mathbf{V}}}\n",
    "\\newcommand{\\Ha}{{\\mathbf{H}}}\n",
    "\\newcommand{\\Wa}{{\\mathbf{W}}} \n",
    "\\newcommand{\\vel}{{\\mathbf M}}\n",
    "\\newcommand{\\malpha}{{\\mathbf A}}\n",
    "\\newcommand{\\vapprox}{{\\mathbf \\Lambda}}$$ \n",
    "\n",
    "### Notations\n",
    "\n",
    "- $\\Va \\approx  \\Wa\\Ha$, where $\\Wa =(w_{fk})_{f,k}  \\in \\mathbb{R}_{+}^{F \\times K}$ and $\\Ha =(h_{kn})_{k,n} \\in \\mathbb{R}_{+}^{K \\times N}$ are interpreted as the nonnegative audio spectral patterns and their activation matrices respectively.\n",
    "- $\\vel \\in  \\mathbb{R}_+^{N \\times C}$: Velocity matrix with each velocity vector arranged into columns as $[m_1 ~ m_2 \\ldots m_C]$\n",
    "\t\n",
    "- $\\malpha \\in \\mathbb{R}_+^{K \\times C}$: Nonnegative weight vector for taking linear combinations of $\\Ha$, with each column denoted by $\\alpha_c$ where $c \\in \\{1,C\\}$ \n",
    "\t\n",
    "\n",
    "\n",
    "\n",
    "### Derivation\n",
    "\n",
    "We formulate the following cost function as:\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& C(\\Wa, \\Ha, \\malpha)&= D_{KL} (\\Va|\\Wa\\Ha) + \\frac{\\lambda}{2}\\|\\vel - \\Ha^\\intercal \\malpha\\|_2^2 + \\mu \\|\\malpha\\|_1\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "where $D_{KL}(.|.)$ is the Kullback-Leibler divergence and $\\lambda$ is a regularization parameter.\n",
    "Note that it is trivial to minimize the cost function in absence of scaling constraints:  $C(\\eta\\Wa, \\Ha/\\eta, \\malpha\\eta) < C(\\Wa, \\Ha, \\malpha)$. Taking $\\eta$ close to zero would lead to degenerate solutions. Therefore, we constrain the columns of $\\Wa$ to have unit norm i.e. we construct $\\widetilde{\\Wa} = \\left[\\frac{w_{1}}{\\|w_{1}\\|}~ \\frac{w_{2}}{\\|w_{2}\\|} \\ldots \\frac{w_{K}}{\\|w_{K}\\|} \\right]$ and incorporate this into the cost function as: \n",
    "\n",
    "\\begin{equation}\n",
    "\\label{cost-joint}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\Wa, \\Ha, \\malpha}{\\text{minimize}}\n",
    "& & \\underbrace{D_{KL} (\\Va|\\widetilde{\\Wa}\\Ha)}_{\\text{Audio Factorization}} + \\underbrace{\\frac{\\lambda}{2}\\|\\vel - \\Ha^\\intercal \\malpha\\|_2^2 + \\mu \\|\\malpha\\|_1}_{\\text{Sparse NNLS}}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "For convenience we will use $\\Lambda = \\widetilde{\\Wa}\\Ha$.\n",
    "\n",
    "#### Preliminaries\n",
    "NMF multiplicative updates can be obtained using the following heuristic approach: the derivative of the \n",
    "$\\beta$--divergence with respect to a particular coefficient $\\gamma$ of $\\mathbf{W}$ or $\\mathbf{H}$ can be written as \n",
    "a difference of two nonnegative functions, $\\nabla D (\\gamma) = \\nabla D ^{+}(\\gamma) - \\nabla D ^{-}(\\gamma)$. The update can then be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{murule}\n",
    "    \\gamma \\leftarrow \\gamma\\frac{\\nabla D ^{-}(\\gamma)}{\\nabla D ^{+}(\\gamma)}\n",
    "\\end{equation}\n",
    "\n",
    "**Note**: A more principled approach to arrive at a solution for such optimization problems is provided by the \n",
    "majorization--minimization technique. This requires construction and minimization of an auxiliary function that is an upper--bound of the original cost function, tight at the current value of the iterate ($\\mathbf{W}$ or $\\mathbf{H}$). By construction, this ensures  that the objective function decreases in each iteration. Interestingly, the resulting updates are multiplicative  and same as those given in equation for $\\beta \\in [1,2]$.\n",
    "\n",
    "\n",
    "#### Update for $\\Ha$\n",
    "Referring to the factorization part of the cost function as $C_{\\text{NMF}}$ and the sparse regression part as $C_{\\text{SLS}}$, the derivative with respect to $\\Ha$ gives us the following negative $\\left[.\\right]_-$ and positive $\\left[.\\right]_+$ components: \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\left[\\nabla_{\\Ha}C_{\\text{NMF}}\\right]_+& = \\widetilde{\\Wa}^\\intercal \\mathbf{1} \\\\\n",
    "\\left[\\nabla_{\\Ha}C_{\\text{NMF}}\\right]_-& = \\widetilde{\\Wa}^\\intercal \\left(\\Va \\odot \\vapprox^{-1}\\right)\\\\\n",
    "\\left[\\nabla_{\\Ha}C_{\\text{SLS}}\\right]_+& = \\malpha \\malpha^\\intercal\\Ha\\\\\n",
    "\\left[\\nabla_{\\Ha}C_{\\text{SLS}}\\right]_-& =  \\malpha \\vel^\\intercal\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Using the heuristic rule to derive NMF updates, we can write the update for $\\Ha$ as:\n",
    "\\begin{equation}\n",
    "\\Ha \\leftarrow \\Ha \\odot \\frac{\\widetilde{\\Wa}^\\intercal \\left(\\Va \\odot \\vapprox^{-1}\\right) + \\lambda \\malpha \\vel^\\intercal}{ \\widetilde{\\Wa}^\\intercal \\mathbf{1} + \\lambda \\malpha \\malpha^\\intercal\\Ha}\n",
    "\\end{equation}\t\n",
    "\n",
    "\n",
    "\n",
    "#### Update for $\\Wa$\n",
    "For this we follow (Le Roux et al., 2015). The update can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Wa \\leftarrow \\Wa \\odot \\cfrac{(\\vapprox^{-1} \\odot \\Va)\\Ha^\\intercal + \\widetilde{\\Wa} \\odot \\left(\\mathbf{1} (\\widetilde{\\Wa} \\odot (\\mathbf{1} \\Ha^\\intercal)\\right)}{\\mathbf{1} \\Ha^\\intercal + \\widetilde{\\Wa} \\odot \\left(\\mathbf{1}(\\widetilde{\\Wa} \\odot ((\\vapprox^{-1} \\odot \\Va)\\Ha^\\intercal))\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "#### Update for $\\malpha$\n",
    "Similar to updates for $\\Ha$, for $\\malpha$ they are easily derived by using $C_{\\text{SLS}}$ and the heuristic:\n",
    "\n",
    "\\begin{equation}\n",
    "\\malpha \\leftarrow \\malpha \\odot \\frac{\\lambda \\Ha \\vel}{\\lambda\\Ha \\Ha^\\intercal \\malpha + \\mu}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy\n",
    "import resampy\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "from oct2py import octave\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def periodic_hann(window_length):\n",
    "    return 0.5 - (0.5 * np.cos(2 * np.pi / window_length *\n",
    "                             np.arange(window_length)))\n",
    "def nnrandn(shape):\n",
    "    return np.abs(np.random.randn(*shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and parameters\n",
    "\n",
    "# --- Paths\n",
    "root_dir = '.'\n",
    "file_path = os.path.join(root_dir,'input_files')\n",
    "audio_path = os.path.join(file_path,'mix.wav')\n",
    "\n",
    "octave_path = os.path.join(root_dir,'octave')\n",
    "octave.addpath(octave_path)\n",
    "\n",
    "# --- STFT params\n",
    "n_fft = 4096\n",
    "hop_size = n_fft//4\n",
    "\n",
    "\n",
    "# --- NMF_NNLS params\n",
    "K = 30 # audio decomposition components\n",
    "l = 0.01 # cross-modal hyper-parameter\n",
    "mu = 0.1 # sparsity hyper-parameter\n",
    "niter = 100\n",
    "\n",
    "display = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview and intuition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"input_files/tutorial_video.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get audio representation matrix V from the mixture audio\n",
    "print('----Loading audio and computing spectrogram----')\n",
    "\n",
    "# --- Load audio mixture and compute spectrogram\n",
    "x, sr = librosa.load(audio_path, sr=None)\n",
    "S = librosa.stft(x, n_fft=n_fft, hop_length=hop_size, window=periodic_hann(n_fft))\n",
    "V = np.abs(S)\n",
    "print('shape of input mixture spectrogram is:', V.shape)\n",
    "\n",
    "if display:\n",
    "    D = librosa.amplitude_to_db(V, ref=np.max)\n",
    "    librosa.display.specshow(D, y_axis='linear', x_axis='time', sr=sr, hop_length=hop_size)\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Input mixture spectrogram')\n",
    "\n",
    "print('----Computing NMF to get initial W and H estimates----')\n",
    "\n",
    "# --- Perform an NMF decomposition for initializing W and H\n",
    "nmf_args = {'beta_loss': 'kullback-leibler', 'solver': 'mu', 'max_iter': 100, 'init': 'random'}  \n",
    "Wini, Hini = librosa.decompose.decompose(V, n_components=K, **nmf_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct motion matrix representation\n",
    "\n",
    "print('----Loading segment velocities for each instrument----')\n",
    "# --- Load segment velocities \n",
    "load_vel_vln1 = scipy.io.loadmat(os.path.join(file_path,'segvel_vln1.mat'))\n",
    "load_vel_vln2 = scipy.io.loadmat(os.path.join(file_path,'segvel_vln2.mat'))\n",
    "\n",
    "vel_raw_src1 = load_vel_vln1['vln1']\n",
    "vel_raw_src2 = load_vel_vln2['vln2']\n",
    "\n",
    "# --- Resample velocity to match time frames in V\n",
    "fps = 30\n",
    "vel_res_src1 = resampy.resample(vel_raw_src1.T, fps, np.ceil(sr/hop_size), axis=0)\n",
    "vel_res_src2 = resampy.resample(vel_raw_src2.T, fps, np.ceil(sr/hop_size), axis=0)\n",
    "\n",
    "vel = np.hstack((vel_res_src1, vel_res_src2)) # N x C\n",
    "vel_src_map = np.hstack((np.ones(vel_raw_src1.shape[0]),2*np.ones(vel_raw_src2.shape[0])))\n",
    "\n",
    "print('shape of motion matrix is:', vel.shape)\n",
    "\n",
    "\n",
    "# Initialize cross-modal assignment matrix A\n",
    "C = vel.shape[1]\n",
    "Aini = nnrandn((K,C))\n",
    "print('shape of cross-modal assignment matrix is:', Aini.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----Performing NMF_NNLS----')\n",
    "# Perform NMF_NNLS\n",
    "W, H, A, cost = octave.NMF_NNLS(V, vel, Wini, Hini, Aini, l, mu, niter, nout=4)\n",
    "\n",
    "\n",
    "print('----Selecting components and reconstructing sources')\n",
    "# Select components and reconstruct sources using soft mask\n",
    "component_cluster = vel_src_map[np.argmax(A,axis=1)]\n",
    "\n",
    "sm_src1 = np.divide(np.matmul(W[:, component_cluster==1],H[component_cluster==1,:]), V)\n",
    "sm_src2 = np.divide(np.matmul(W[:, component_cluster==2],H[component_cluster==2,:]), V)\n",
    "\n",
    "stft_src1 = np.multiply(sm_src1,S)\n",
    "stft_src2 = np.multiply(sm_src2,S)\n",
    "\n",
    "recon_src1 = librosa.istft(wf_src1, hop_length=hop_size,window=periodic_hann(n_fft))\n",
    "recon_src2 = librosa.istft(wf_src2, hop_length=hop_size,window=periodic_hann(n_fft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "print('Reconstructed source 1')\n",
    "ipd.display(ipd.Audio(recon_src1, rate=sr))\n",
    "\n",
    "print('Reconstructed source 2')\n",
    "ipd.display(ipd.Audio(recon_src2, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch code, to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## only for testing, remove it later, have a python code now!\n",
    "#wf, _, _ = octave.Component_Selection(W, H, A, vel_src_map, nout=3)\n",
    "\n",
    "#wf_src1 = np.multiply(wf[:,:,0],S)\n",
    "#wf_src2 = np.multiply(wf[:,:,1],S)\n",
    "\n",
    "#recon_src1 = librosa.istft(wf_src1, hop_length=hop_size,window=periodic_hann(n_fft))\n",
    "#recon_src2 = librosa.istft(wf_src2, hop_length=hop_size,window=periodic_hann(n_fft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an intuition for correspondence between motion and audio\n",
    "# plot extracted segment velocities, run with video and audio\n",
    "\n",
    "load_vel_vln1 = scipy.io.loadmat(os.path.join(file_path,'segvel_vln1.mat'))['vln1']\n",
    "fig, axes = plt.subplots(nrows=load_vel_vln1.shape[0], ncols=1, figsize=(10,5), sharey='col')\n",
    "for ax, row in zip(axes.flatten(), load_vel_vln1):\n",
    "    ax.plot(row,  'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
